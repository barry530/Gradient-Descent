{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take height_weight_genders.csv as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and convert it to metric system\n",
    "def load_data(subsample=False, add_outlier = True):\n",
    "    path = 'height_weight_genders.csv'\n",
    "    data = np.genfromtxt(path, delimiter=',', skip_header=True, usecols=[1,2])\n",
    "    height = data[:,0]\n",
    "    weight = data[:,1]\n",
    "    gender = np.genfromtxt(path, delimiter=',', skip_header=True, usecols=[0],\n",
    "                          converters={0: lambda x : 1 if b'Male' in x else 0}) # col 0\n",
    "    height *= 0.025\n",
    "    weight *= 0.454\n",
    "    if subsample:\n",
    "        height = height[::20]\n",
    "        weight = weight[::20]\n",
    "    if add_outlier:\n",
    "        height = np.concatenate([height, [1.1,1.2]])\n",
    "        weight = np.concatenate([weight, [51.5/0.454, 55.3/0.454]])\n",
    "    return height, weight, gender\n",
    "\n",
    "def standerdize(x):\n",
    "    x_mean = np.mean(x, axis=0)\n",
    "    x_std = np.std(x, axis=0)\n",
    "    x = (x - x_mean) / x_std\n",
    "    return x, x_mean, x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, weight, gender = load_data(subsample=False, add_outlier=False)\n",
    "x, x_mean, x_std = standerdize(height)\n",
    "y = weight\n",
    "x_mat = np.c_[np.ones(len(x)),x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Height      Weight  Gender\n",
      " 1.846175  109.819678       1\n",
      " 1.719548   73.688955       1\n",
      " 1.852753   96.584348       1\n",
      " 1.793274   99.899282       1\n",
      " 1.747045   93.682809       1\n"
     ]
    }
   ],
   "source": [
    "DF = pd.DataFrame({'Height':height,'Weight':weight,'Gender':gender})\n",
    "print(DF.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.94406149]\n",
      " [ 1.          0.62753668]\n",
      " [ 1.          2.01244346]\n",
      " ...\n",
      " [ 1.         -0.64968792]\n",
      " [ 1.          0.69312469]\n",
      " [ 1.         -1.14970831]]\n"
     ]
    }
   ],
   "source": [
    "print(x_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(e):\n",
    "    return np.mean(e**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(e):\n",
    "    return np.mean(np.abs(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, x_mat, w):\n",
    "    e = y - x_mat.dot(w)\n",
    "    return mse(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_w(num_intervals):\n",
    "    w0 = np.linspace(-100,250,num_intervals)\n",
    "    w1 = np.linspace(-150,120,num_intervals)\n",
    "    return w0, w1\n",
    "\n",
    "def grid_search(y, x_mat, w0, w1): #find the lowest loss\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    for i, row in enumerate(w0):\n",
    "        for j, col in enumerate(w1):\n",
    "            w = np.array([row, col])\n",
    "            losses[i, j] = loss(y, x_mat, w)\n",
    "    return losses\n",
    "\n",
    "def best_param(w0, w1, losses):\n",
    "    min_row, min_col = np.unravel_index(np.argmin(losses), losses.shape)\n",
    "    return losses[min_row, min_col], w0[min_row], w1[min_col]\n",
    "\n",
    "def prediction(w0, w1, x_mean, x_std):\n",
    "    x = np.arange(1.2, 2, 0.01)\n",
    "    x_normalised = (x - x_mean) / x_std\n",
    "    return x, w0 + w1 * x_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_visualisation(grid_losses, w0_list, w1_list, x_mean, x_std, height, weight):\n",
    "    w0, w1 = np.meshgrid(w0_list, w1_list)\n",
    "    loss_star, w0_star, w1_star = best_param(w0_list, w1_list, grid_losses)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    cp = ax1.contourf(w0, w1, grid_losses.T, cmap = plt.cm.jet)\n",
    "    fig.colorbar(cp, ax=ax1)\n",
    "    ax1.set_xlabel(r'$w_0$')\n",
    "    ax1.set_ylabel(r'$w_1$')\n",
    "    ax1.plot(w0_star, w1_star, marker = '*', color = 'r', markersize = 20)\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.scatter(height, weight, marker='.', color = 'b', s = 5)\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "    ax2.grid()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_visualisation(gradient_losses, gradient_w,\n",
    "                                   grid_losses, grid_w0, grid_w1,\n",
    "                                   x_mean, x_std, height, weight, n_iter=None):\n",
    "    fig = base_visualisation(grid_losses, grid_w0, grid_w1, x_mean, x_std, height, weight)\n",
    "    w_plotted = np.stack(gradient_w)\n",
    "    if n_iter is not None:\n",
    "        w_plotted = w_plotted[:n_iter]\n",
    "    \n",
    "    ax1, ax2 = fig.get_axes()[0], fig.get_axes()[2]\n",
    "    ax1.plot(w_plotted[:,0], w_plotted[:,1], marker='o', color='w', markersize=5)\n",
    "    pred_x, pred_y = prediction(w_plotted[-1,0], w_plotted[-1,1], x_mean, x_std)\n",
    "    ax2.plot(pred_x, pred_y, 'r')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "## Gradient\n",
    "Suppose $y \\approx f(x) = w_0+w_1x = \\mathbf{X}w$  \n",
    "$ \\mathbf{X} \\in R^{n \\times (m+1)}, w \\in R^{m \\times 1}$  \n",
    "$error = y-\\mathbf{X}w$  \n",
    "Here we take MSE as examples:  \n",
    "$L = \\frac{\\sum (y-\\mathbf{X}w)^2}{n}$  \n",
    "then the gradient of the cost is:  \n",
    "$\\Delta L = \\frac{\\partial L}{\\partial w} = -\\frac{2}{n}\\mathbf{X}^Te$\n",
    "\n",
    "## Gradient Descent\n",
    "$w^{k+1} = w^k - \\tau \\Delta L(w^k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(y, x_mat, w):\n",
    "    err = y - x_mat.dot(w)\n",
    "    grad = -2 * x_mat.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "def gradient_descent(y, x_mat, tau, w_init, max_iter):\n",
    "    w_ = [w_init]\n",
    "    losses = []\n",
    "    w = w_init\n",
    "    for i in range(max_iter):\n",
    "        gradient, err = grad(y, x_mat, w)\n",
    "        loss = mse(err)\n",
    "        w = w - tau * gradient\n",
    "        w_.append(w)\n",
    "        losses.append(loss)\n",
    "        if (i+1)%5 == 0:\n",
    "            print('Gradient Descent Iteration ({a}/{b}) : loss = {l}, w0 = {w0}, w1 = {w1}'.format(\n",
    "                a = i+1, b = max_iter, l = loss.round(7), w0=w[0].round(7), w1=w[1].round(7)))\n",
    "    return losses, w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=30.998806863732035, w0*=72.89156626506022, w1*=13.734939759036138\n"
     ]
    }
   ],
   "source": [
    "w0_grid, w1_grid = generate_w(num_intervals=250)\n",
    "grid_losses = grid_search(y, x_mat, w0_grid, w1_grid)\n",
    "lowest_loss, w0_best, w1_best = best_param(w0_grid, w1_grid, grid_losses)\n",
    "print('Grid Search: loss*={l}, w0*={w0}, w1*={w1}'.format(l=lowest_loss, w0=w0_best, w1=w1_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Iteration (5/60) : loss = 2421.4582301, w0 = 30.014594, w1 = 5.520077\n",
      "Gradient Descent Iteration (10/60) : loss = 864.3525994, w0 = 47.7379116, w1 = 8.7796273\n",
      "Gradient Descent Iteration (15/60) : loss = 321.423437, w0 = 58.2033534, w1 = 10.7043592\n",
      "Gradient Descent Iteration (20/60) : loss = 132.1157436, w0 = 64.3830922, w1 = 11.8408941\n",
      "Gradient Descent Iteration (25/60) : loss = 66.1082324, w0 = 68.0321661, w1 = 12.5120066\n",
      "Gradient Descent Iteration (30/60) : loss = 43.0928363, w0 = 70.1869078, w1 = 12.9082918\n",
      "Gradient Descent Iteration (35/60) : loss = 35.0678639, w0 = 71.4592612, w1 = 13.1422943\n",
      "Gradient Descent Iteration (40/60) : loss = 32.2697291, w0 = 72.2105731, w1 = 13.2804704\n",
      "Gradient Descent Iteration (45/60) : loss = 31.2940798, w0 = 72.6542153, w1 = 13.362062\n",
      "Gradient Descent Iteration (50/60) : loss = 30.9538919, w0 = 72.9161816, w1 = 13.410241\n",
      "Gradient Descent Iteration (55/60) : loss = 30.8352757, w0 = 73.0708701, w1 = 13.4386903\n",
      "Gradient Descent Iteration (60/60) : loss = 30.7939168, w0 = 73.1622121, w1 = 13.4554893\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([0, 0])\n",
    "gradient_losses, gradient_w = gradient_descent(y, x_mat, 0.05, w_init, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59a04aa53c842278ef2b1465728e1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=61, min=1, step=20), Output()), _dom_classe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualisation(\n",
    "        gradient_losses, gradient_w, grid_losses, grid_w0, grid_w1, x_mean, x_std, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_w), step=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "ref : https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/  \n",
    "ref : https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(y, x_mat, batch_size, num_batches, shuffle=True):\n",
    "    n = len(y)\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(n)\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_xmat = x_mat[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_xmat = x_mat\n",
    "    \n",
    "    for num_batch in range(num_batches):\n",
    "        start_index = num_batch * batch_size\n",
    "        end_index = min((num_batch + 1) * batch_size, n)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_xmat[start_index:end_index]\n",
    "\n",
    "def grad_stoch(y, x_mat, w):\n",
    "    err = y - x_mat.dot(w)\n",
    "    grad = -2*x_mat.T.dot(err) / len(err)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stochastic_Gradient_Descent(y, x_mat, w_init, batch_size, num_batches, max_iters, tau):\n",
    "    w = w_init\n",
    "    losses = []\n",
    "    w_ = [w_init]\n",
    "    for iter in range(max_iters):\n",
    "        for y_batch, x_batch in batches_generator(y, x_mat, batch_size=batch_size, num_batches=num_batches):\n",
    "            gradient, err = grad_stoch(y_batch, x_batch, w)\n",
    "            w = w - tau * gradient\n",
    "            error = y - x_mat.dot(w)\n",
    "            loss = mse(error)\n",
    "            w_.append(w)\n",
    "            losses.append(loss)\n",
    "        if (iter+1)%10 == 0:\n",
    "            print('SGD({a}/{b}) : loss = {l}, w = {w}'.format(\n",
    "                    a = iter+1, b = max_iters, l = loss, w = w))\n",
    "    return losses, w_\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(10/50) : loss = 31.743098414021357, w = [72.50153946 14.06576071]\n",
      "SGD(20/50) : loss = 41.364219721618106, w = [70.64961007 11.58233045]\n",
      "SGD(30/50) : loss = 46.34647929591696, w = [72.57143602  9.59992925]\n",
      "SGD(40/50) : loss = 62.891510866353485, w = [67.72152716 12.44619859]\n",
      "SGD(50/50) : loss = 76.02942963316622, w = [76.68616692 19.28921571]\n"
     ]
    }
   ],
   "source": [
    "sgd_losses, sgd_w = Stochastic_Gradient_Descent(y, x_mat, w_init, 1, 1, 50, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded10ab7a0e349deae1a39b64f235c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1, step=25), Output()), _dom_classe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualisation(\n",
    "        sgd_losses, sgd_w, grid_losses, grid_w0, grid_w1, x_mean, x_std, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_w), step=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniBatch_Gradient_Descent(y, x_mat, w_init, batch_size, num_batches, max_iters, tau):\n",
    "    w = w_init\n",
    "    losses = []\n",
    "    w_ = [w_init]\n",
    "    for iter in range(max_iters):\n",
    "        for y_batch, x_batch in batches_generator(y, x_mat, batch_size=batch_size, num_batches=num_batches):\n",
    "            gradient, err = grad_stoch(y_batch, x_batch, w)\n",
    "            w = w - tau * gradient\n",
    "            error = y - x_mat.dot(w)\n",
    "            loss = mse(error)\n",
    "            w_.append(w)\n",
    "            losses.append(loss)\n",
    "        if (iter+1)%10 == 0:\n",
    "            print('Mini-Batch Gradient Descent({a}/{b}) : loss = {l}, w = {w}'.format(\n",
    "                    a = iter+1, b = max_iters, l = loss, w = w))\n",
    "    return losses, w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-Batch Gradient Descent(10/50) : loss = 659.8360274565425, w = [48.4104024  10.33730829]\n",
      "Mini-Batch Gradient Descent(20/50) : loss = 104.4053543590906, w = [65.00421908 11.2628669 ]\n",
      "Mini-Batch Gradient Descent(30/50) : loss = 43.15885382047828, w = [70.05411178 12.10468233]\n",
      "Mini-Batch Gradient Descent(40/50) : loss = 32.38258800736659, w = [72.15278517 12.92417756]\n",
      "Mini-Batch Gradient Descent(50/50) : loss = 31.29569462361704, w = [72.5976721  13.67758857]\n"
     ]
    }
   ],
   "source": [
    "mbgd_losses, mbgd_w = MiniBatch_Gradient_Descent(y, x_mat, w_init, 10, 1, 50, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284d56cf7719421089996d30c7c4fb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1, step=25), Output()), _dom_classe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualisation(\n",
    "        mbgd_losses, mbgd_w, grid_losses, grid_w0, grid_w1, x_mean, x_std, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(mbgd_w), step=25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
