{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take height_weight_genders.csv as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and convert it to metric system\n",
    "def load_data(subsample=False, add_outlier = True):\n",
    "    path = 'height_weight_genders.csv'\n",
    "    data = np.genfromtxt(path, delimiter=',', skip_header=True, usecols=[1,2])\n",
    "    height = data[:,0]\n",
    "    weight = data[:,1]\n",
    "    gender = np.genfromtxt(path, delimiter=',', skip_header=True, usecols=[0],\n",
    "                          converters={0: lambda x : 1 if b'Male' in x else 0}) # col 0\n",
    "    height *= 0.025\n",
    "    weight *= 0.454\n",
    "    if subsample:\n",
    "        height = height[::20]\n",
    "        weight = weight[::20]\n",
    "    if add_outlier:\n",
    "        height = np.concatenate([height, [1.1,1.2]])\n",
    "        weight = np.concatenate([weight, [51.5/0.454, 55.3/0.454]])\n",
    "    return height, weight, gender\n",
    "\n",
    "def standerdize(x):\n",
    "    x_mean = np.mean(x, axis=0)\n",
    "    x_std = np.std(x, axis=0)\n",
    "    x = (x - x_mean) / x_std\n",
    "    return x, x_mean, x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, weight, gender = load_data(subsample=False, add_outlier=False)\n",
    "x, x_mean, x_std = standerdize(height)\n",
    "y = weight\n",
    "x_mat = np.c_[np.ones(len(x)),x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Height      Weight  Gender\n",
      " 1.846175  109.819678       1\n",
      " 1.719548   73.688955       1\n",
      " 1.852753   96.584348       1\n",
      " 1.793274   99.899282       1\n",
      " 1.747045   93.682809       1 (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "DF = pd.DataFrame({'Height':height,'Weight':weight,'Gender':gender})\n",
    "print(DF.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.94406149],\n",
       "       [ 1.        ,  0.62753668],\n",
       "       [ 1.        ,  2.01244346],\n",
       "       ...,\n",
       "       [ 1.        , -0.64968792],\n",
       "       [ 1.        ,  0.69312469],\n",
       "       [ 1.        , -1.14970831]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(e):\n",
    "    return np.mean(e**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(e):\n",
    "    return np.mean(np.abs(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, x_mat, w):\n",
    "    e = y - x_mat.dot(w)\n",
    "    return mse(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_w(num_intervals):\n",
    "    w0 = np.linspace(-100,250,num_intervals)\n",
    "    w1 = np.linspace(-150,120,num_intervals)\n",
    "    return w0, w1\n",
    "\n",
    "def grid_search(y, x_mat, w0, w1): #find the lowest loss\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    for i, row in enumerate(w0):\n",
    "        for j, col in enumerate(w1):\n",
    "            w = np.array([row, col])\n",
    "            losses[i, j] = loss(y, x_mat, w)\n",
    "    return losses\n",
    "\n",
    "def best_param(w0, w1, losses):\n",
    "    min_row, min_col = np.unravel_index(np.argmin(losses), losses.shape)\n",
    "    return losses[min_row, min_col], w0[min_row], w1[min_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=30.998806863732035, w0*=72.89156626506022, w1*=13.734939759036138\n"
     ]
    }
   ],
   "source": [
    "w0_grid, w1_grid = generate_w(250)\n",
    "grid_losses = grid_search(y, x_mat, w0_grid, w1_grid)\n",
    "lowest_loss, w0_best, w1_best = best_param(w0_grid, w1_grid, grid_losses)\n",
    "print('Grid Search: loss*={l}, w0*={w0}, w1*={w1}'.format(l=lowest_loss, w0=w0_best, w1=w1_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[75635.14279993, 75288.17108514, 74943.21546685, ...,\n",
       "        51184.15745377, 51335.16157807, 51488.18179888],\n",
       "       [75122.1854438 , 74775.213729  , 74430.25811072, ...,\n",
       "        50671.20009763, 50822.20422193, 50975.22444275],\n",
       "       [74612.13126664, 74265.15955184, 73920.20393356, ...,\n",
       "        50161.14592048, 50312.15004478, 50465.17026559],\n",
       "       ...,\n",
       "       [37136.15645563, 36789.18474083, 36444.22912254, ...,\n",
       "        12685.17110946, 12836.17523376, 12989.19545457],\n",
       "       [37340.28430779, 36993.31259299, 36648.35697471, ...,\n",
       "        12889.29896163, 13040.30308593, 13193.32330674],\n",
       "       [37547.31533894, 37200.34362414, 36855.38800586, ...,\n",
       "        13096.32999277, 13247.33411707, 13400.35433789]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_losses.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w0, w1 = np.meshgrid(w0_grid, w1_grid)\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "cp = ax1.contourf(w0, w1, grid_losses.T, cmap = plt.cm.jet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-100.        ,  -98.99598394,  -97.99196787, ...,  147.99196787,\n",
       "         148.99598394,  150.        ],\n",
       "       [-100.        ,  -98.99598394,  -97.99196787, ...,  147.99196787,\n",
       "         148.99598394,  150.        ],\n",
       "       [-100.        ,  -98.99598394,  -97.99196787, ...,  147.99196787,\n",
       "         148.99598394,  150.        ],\n",
       "       ...,\n",
       "       [-100.        ,  -98.99598394,  -97.99196787, ...,  147.99196787,\n",
       "         148.99598394,  150.        ],\n",
       "       [-100.        ,  -98.99598394,  -97.99196787, ...,  147.99196787,\n",
       "         148.99598394,  150.        ],\n",
       "       [-100.        ,  -98.99598394,  -97.99196787, ...,  147.99196787,\n",
       "         148.99598394,  150.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "## Gradient\n",
    "Suppose $y \\approx f(x) = w_0+w_1x$  \n",
    "$error = y-\\mathbf{X}w$  \n",
    "Here we take MSE as examples:  \n",
    "$L = \\frac{\\sum (y-\\mathbf{X}w)^2}{n}$  \n",
    "then the gradient of the cost is:  \n",
    "$\\Delta L = \\frac{\\partial L}{\\partial w} = -\\frac{2}{n}\\mathbf{X}^Te$\n",
    "\n",
    "## Gradient Descent\n",
    "$w^{k+1} = w^k - \\tau \\Delta L(w^k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(y, x_mat, w):\n",
    "    err = y - x_mat.dot(w)\n",
    "    grad = -2*x_mat.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "def gradient_descent(y, x_mat, tau, w_init, max_iter):\n",
    "    w_ = [w_init]\n",
    "    losses = []\n",
    "    w = w_init\n",
    "    for i in range(max_iter):\n",
    "        gradient, err = grad(y, x_mat, w)\n",
    "        loss = mse(err)\n",
    "        w = w - tau * gradient\n",
    "        w_.append(w)\n",
    "        losses.append(loss)\n",
    "        #print('Gradient Descent Iteration ({a}/{b}) : loss = {l}, w0 = {w0}, w1 = {w1}'.format(\n",
    "         #       a = i+1, b = max_iter, l = loss, w0=w[0], w1=w[1]))\n",
    "    return losses, w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2041.6862728588699,\n",
       " 1397.4853606363195,\n",
       " 965.299286561489,\n",
       " 675.3035059403763,\n",
       " 480.6703301192994,\n",
       " 349.9948450092105,\n",
       " 262.215375802553,\n",
       " 203.20691168891716,\n",
       " 163.49649622776266,\n",
       " 136.73106065870883,\n",
       " 118.64988083458232,\n",
       " 106.39545848946076,\n",
       " 98.05136578277487,\n",
       " 92.3323020911869,\n",
       " 88.37623149304034,\n",
       " 85.60498036669965,\n",
       " 83.63074756149348,\n",
       " 82.19340569962459,\n",
       " 81.11845245909765,\n",
       " 80.28881070554817]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([10,20]).reshape(2,-1)\n",
    "y = np.array([20,26,17,14,16,19,14,28,25,22]).reshape(10,1)\n",
    "x = np.random.randn(10,2)+2\n",
    "gradient_descent(y, x, 0.01, w, 20)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "ref : https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/  \n",
    "ref : https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butches_generator(y, x_mat, batch_size, num_batches, shuffle=True):\n",
    "    n = len(y)\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(n)\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_xmat = x_mat[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_xmat = x_mat\n",
    "    \n",
    "    for num_batch in range(num_batches):\n",
    "        start_index = num_batch * batch_size\n",
    "        end_index = min((num_batch + 1) * batch_size, n)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_xmat[start_index:end_index]\n",
    "\n",
    "def grad_stoch(y, x_mat, w):\n",
    "    err = y - x_mat.reshape(len(y),-1).dot(w)\n",
    "    grad = -2*x_mat.reshape(len(y),-1).T.dot(err) / len(err)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([45, 36]), array([[ 1,  2],\n",
       "        [11,  2]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2,3,4,5,6,7,8,9,10],[6,7,11,2,6,8,4,2,6,9]]).reshape(10,2)\n",
    "y = np.array([45,73,45,34,21,16,36,41,34,12])\n",
    "a = butches_generator(y, x, 2, 5)\n",
    "next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniBatch_Gradient_Descent(y, x_mat, w_init, batch_size, num_batches, epochs, tau):\n",
    "    w = w_init\n",
    "    losses = []\n",
    "    w_ = [w_init]\n",
    "    for iter in range(epochs):\n",
    "        butches = butches_generator(y, x_mat, batch_size, num_batches)\n",
    "        batch_y, batch_x = next(butches)\n",
    "        n = len(batch_y)\n",
    "        gradients = []\n",
    "        for i in range(n):\n",
    "            gradient, err = grad_stoch(batch_y[i], batch_x[i], w)\n",
    "            gradients.append(gradient)\n",
    "            gradient_update = np.mean(gradients)\n",
    "        w = w - tau * gradient_update\n",
    "        error = y - x_mat.dot(w)\n",
    "        loss = mse(error)\n",
    "        w_.append(w)\n",
    "        losses.append(loss)\n",
    "        #print('MiniBatch_Gradient_Descent epoch ({a}/{b}) : loss = {l}, w = {w}'.format(\n",
    "         #       a = iter+1, b = epochs, l = loss, w = w))\n",
    "    return losses, w_\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1111.8534243759527,\n",
       "  783.5267263168588,\n",
       "  458.6630214245767,\n",
       "  357.92484591468025,\n",
       "  304.4403063958374,\n",
       "  250.9456094723912,\n",
       "  195.04741000823861,\n",
       "  157.8245486937935,\n",
       "  140.64341590054576,\n",
       "  125.67555868502488],\n",
       " [array([[10],\n",
       "         [20]]), array([[ 8.46838907],\n",
       "         [18.46838907]]), array([[ 7.04284321],\n",
       "         [17.04284321]]), array([[ 5.21976619],\n",
       "         [15.21976619]]), array([[ 4.48532026],\n",
       "         [14.48532026]]), array([[ 4.03249695],\n",
       "         [14.03249695]]), array([[ 3.51045849],\n",
       "         [13.51045849]]), array([[ 2.83969597],\n",
       "         [12.83969597]]), array([[ 2.24989306],\n",
       "         [12.24989306]]), array([[ 1.89114604],\n",
       "         [11.89114604]]), array([[ 1.46762209],\n",
       "         [11.46762209]])])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([10,20]).reshape(2,-1)\n",
    "y = np.array([20,26,17,14,16,19,14,28,25,22]).reshape(10,1)\n",
    "x = np.random.randn(10,2)+2\n",
    "MiniBatch_Gradient_Descent(y, x, w, 4, 5, 10, 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
